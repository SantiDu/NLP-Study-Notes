%!TEX program = xelatex
\documentclass{report}

\title{NLP Study Notes}
\author{Du Jinrui}
\date{2022} \usepackage[b6paper, margin=0.5in]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[UTF8]{ctex}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
  \maketitle
  \tableofcontents
  \chapter{Shortest-Path Algorithms and Dynamic Programming}
  \section{Graphs}
  \section{Dynamic programming}
  When designing a DP algorithm, there are two things to consider:
  \begin{enumerate}
  	\item Deconstruct a big problem into smaller (recursive) sub-problems.
	\item Store intermediate results.
  \end{enumerate}
  \subsection{DP coding problems}
  \begin{itemize}
    \item Nth Fibonacci Number
    \item Longest Increasing Sub-sequence
    \item Coin Change
  \end{itemize}
  \section{Dynamic time warping}
  \section{The Viterbi algorithm}
  \chapter{Logistic Regression}
  \section{The importance of establishing a baseline}
  We draw a function that shows decreased marginal accuracy with increasing model complexity. From this graph, we observe an upper limit. This limit helps us making informed decisions like:
		\begin{enumerate}
			\item Is this project feasible? (the requirement is 75\% accuracy but the upper limit is 72\%.)
			\item Is it cost-effective to add model complexity?
		\end{enumerate}
Furthermore, if we use a complex model upfront without setting a baseline but the accuracy is bad, then it's hard for us to tell whether there was a mistake when building the model or it's because the problem is too complex.

  \section{Understanding LR}
  graph of 1d data draft*

  \href{https://towardsdatascience.com/why-sigmoid-a-probabilistic-perspective-42751d82686}{Why sigmoid?}
  \section{From likelihood to cost function}
  The likelihood function is defined as $l\left( \theta|D \right) = f\left( D|\theta\right) $. $f$ can be either a PMF or a PDF. $|$ is used instead of $;$ because we employ the Bayesian view (not frequenist) and see $\theta$ as a random variable. $l$ is a function of $\theta$ and doesn't integrate to 1 (with respect to $\theta$).

The likelihood function of logistic regression is 
\begin{displaymath}
\prod_{i=1}^{n} {\sigma\left( wx_{i} + b \right)^{y_{i}}\left( 1 - \sigma\left( wx_{i} + b\right)\right)^{1 - y_{i} }}.
\end{displaymath} (\href{https://zstevenwu.com/courses/s20/csci5525/resources/slides/lecture05.pdf}{see derivation})
Maximizing the likelihood is equal to minimizing the negative log-likelihood: \[
	cost\left( w, b \right) = -\sum_{i=1}^{n} y_{i} \ln \sigma \left(wx_{i} + b\right) + \left( 1 - y_{i} \right) \ln \left(1 - \sigma\left(wx_{i} + b\right)\right)
.\] 
And we get KL divergence, or binary cross-entropy, which is convex. (Why is it convex?)

\subsection{On KL-divergence and cross-entropy}
\subsection{Logistic loss}
If the outcome space is $y=\{-1, 1\}$ instead of  $y=\{0, 1\}$, then
\begin{align*}
	p(y_i = 1 | f(x_i)) &= \sigma(f(x_i)) = \frac{1}{1 + e^{-f(x_i)}} \\
	p(y_i = -1 | f(x_i)) &= 1 - \sigma(f(x_i)) = \frac{1}{1 + e^{f(x_i)}}
.\end{align*}
In both cases \[
	p(y_i | f(x_i)) = \frac{1}{1 + e^{-y_if(x_i)}}
.\]  
The negative log-likelihood is \[
	\sum_{i=1}^{n} \log(1 + e^{-y_if(x_i)})
.\] 
Which is called the log loss/logistic loss and it's the same thing as the cross-entropy loss.

  \section{Implement LR with mini-batch GD}The cost function can't be solved analytically, hence we use gradient descent.
  The derivative of the sigmoid function is:
  \[
  \sigma(x)(1 - \sigma(x))
  .\] 
  Knowing this facilitates the calculation of the gradient: 
  \begin{align*}
	  \frac{\partial l(w, b)}{\partial w} & = \sum_{i = 1}^{n} (\sigma(wx_i + b) - y_i) x_i \\
\frac{\partial l(w, b)}{\partial b} & = \sum_{i = 1}^{n} \sigma(wx_i + b) - y_i. 
  \end{align*}
  Now we update the parameters:
  \begin{align*}
    w^{t+1} &= w^t - \eta_t \sum_{i=1}^{n} (\sigma(wx_i + b) - y_i) x_i \\
    b^{t+1} &= b^t - \eta_t \sum_{i=1}^{n} \sigma(wx_i + b) - y_i
  .\end{align*}
Now we've got the updates using GD. The updates using mini-batch GD and stochastic GD become apparent. The former is:
  \begin{align*}
    w^{t+1} &= w^t - \eta_t \sum_{x_i, y_i \in batch}(\sigma(wx_i + b) - y_i) x_i \\
    b^{t+1} &= b^t - \eta_t \sum_{x_i, y_i \in batch}\sigma(wx_i + b) - y_i
  .\end{align*}

  Between GD and stochastic GD, mini-batch GD finds the balance between robustness and efficiency. Moreover, it works well with GPU, and it helps escaping the saddle point. 

  code draft*
  \chapter{Generalization}
  \section{When w goes to infinity}
  When the problem is linearly separable, as w goes to infinity:
 \begin{align*}
	\lim_{w \to \infty} p\left( y_{i}=1 | x_{i} ; w,b \right) &=  \lim_{w \to \infty} \frac{1}{1 + e^{-(wx_{i} + b)} } = 1 \ for\ wx_i + b > 0,\\
	\lim_{w \to \infty} p( y_{i}=0 | x_{i} ; w,b ) &= \lim_{w \to \infty} \frac{e^{-(wx_{i}+b)}}{1 + e^{-(wx_{i} + b)}} = 0\ for\ wx_i + b < 0 
.\end{align*}
At this time, MLE is the largest:
 \[
	 MLE = \argmax_{w, b} \prod_{i=1}^{n}  p\left( y_{i}=1 | x_{i} ; w,b \right)^{y_{i}}p\left( y_{i}=0 | x_{i} ; w,b \right)^{1 - y_{i}}  
.\] 
It is consistent with our goal of maximizing the likelihood function to aim for a large w. For a linearly separable problem, w doesn't converge, and regularization gives bounded solution.

For a non-linearly separable problem, w can converge (mathematically, why?). But when there are too many features, the non-separable becomes the separable, again, w goes to infinity, and uncertainty regions shrink to 0. At this point, limiting the magnitude of w leads to better generalization  and gives back uncertainty regions. How are all these happening? \href{https://stackoverflow.com/questions/24666312/why-limit-the-weight-size-can-prevent-overfitting-in-machine-learning}{1} \href{https://stackoverflow.com/questions/34569903/how-does-having-smaller-values-for-parameters-help-in-preventing-over-fitting}{2} Graphically, higher degree terms variables with smaller w doesn't disappear, but go `out of range', e.g. $y = 6x_1 + 3x_2^2$ vs $y = 6x_1 + 0.1x_2^2$. draft* 

\href{https://stats.stackexchange.com/q/569381/354019}{We don't discuss feature selection here, why don't we just use feature selection?} Is there an algorithm for separability testing?
\section{L1 and L2 regularization}
3d geometric moving representation of l1 and l2 and why l1 makes some parameters 0. draft*

There are some disadvantages of l1 regularization:
\begin{enumerate}
	\item It's not differentiable everywhere, so gradient descent doesn't work, in this case we can use subgradient descent (I don't need to know the details).
	\item When a group of collinear features exist, it randomly selects one feature, but we want the best feature. The lecturer says using elastic net can counter this problem but I don't know how. It's another topic. draft*
\end{enumerate}

\section{K-fold CV}
When dataset is small, we can increase k. One extreme case is leave-one-out CV.

\section{MLE, MAP and L1, L2}
MLE: \[
	p(D|\theta)
.\] 
MAP:
\[
	p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)} \propto p(D|\theta)p(\theta)
.\] 
MAP estimator: \[
	\theta_{MAP} = \argmax_{\theta}\ prior\ \cdot\ likelihood
.\] 
Assume prior is $p(\theta) \sim N(0, \sigma^2)$,
\begin{align*}
	p(\theta) &= \frac{1}{\sqrt{2\pi} \sigma} \exp \left( -\frac{\theta^2}{2\sigma^2} \right) \\
		  &\propto \exp \left( -\frac{
		  \theta^2}{2\sigma^2} \right), \\
		  \argmax_{\theta}\ \log(p(\theta)) &= \argmax_{\theta}\ \log \left( \exp \left( -\frac{\theta^2}{2\sigma^2} \right) \right) \\ 
 &= \argmax_{\theta}\ -\frac{\theta^2}{2\sigma^2}, \\
		  \theta_{MAP} &= \argmin_{\theta} - \log \ likelihood +  \frac{1}{2\sigma^2} \theta^2
.\end{align*}
This looks very familiar. MAP estimator with Gaussian prior equals adding a l2 regularization term to the cost function (and how does the $\lambda$ coefficient relates to the variance? draft*).

Similarly when $p(\theta) \sim Laplace(0, b)$, the resulting cost function is added by l1 term.

\chapter{Naive Bayes}
\chapter{Theory of Convex Optimization}
\section{Mathematical optimization}
A mathematical optimization problem has the form:
\begin{align*}
	\text{minimize}\ &f_0(x) \\
	\text{subject to}\ &f_i(x) \le  b_i,\ i = 1,2,\ldots,m,
\end{align*}
in which $f_0, f_1, \ldots, f_i$ are functions that map $R^d$ to $R$.

In a linear programming problem,  $f_0, f_1, \ldots, f_i$ are linear, which means \[
	f_i(\alpha x + \beta y) = \alpha f_i(x) + \beta f_i(y).
\] For a convex optimization problem, we have \[
	f_i(\alpha x + \beta y) \le  \alpha f_i(x) + \beta f_i(y)
.\] , with $\alpha \ge 0, \beta \ge 0$ and $\alpha + \beta = 1$.
Comparing linear programming problem and convex optimization problem, because convexity is less restricted, all linear problems are convex optimization problems. (I have problem understanding this, \href{https://math.stackexchange.com/q/4231709/727493}{this link} helps. draft*)
\section{Least-squares, linear programming and convex optimization}
Generally, mathematical optimization problems are hard to solve, but we present 3 exceptions in this section.
\subsection{Least-squares}
\subsection{Linear programming}
\subsection{Convex optimization}
\section{Non-linear optimization}
Non-linear problems are optimization problems that are not linear, but not known to be convex.

Finding a global minima is time-consuming, and finding a local minima is more of an art than a science.

Formulating a non-linear problem is relatively straightforward, but the difficulty lies in solving it. While for a convex optimization problem, solving it is straightforward, but the challenge is in problem formulation. 
\chapter{SVM}
\section{Decision theory and the statistical learning framework}
{\noindent input space: $X$}\\
action space: $A$\\
output space: $y$\\
loss: \[ l: A \times y \to R\]\\
decision function: \[ f: X \to A\] \\
risk: \[R(f) = E[l(f(x), y)] \]
Bayesian decision function: \[ f^{\ast} = \argmin_{f} R(f) \]
empirical risk: \[
\hat{R}_n(f) = \frac{1}{n} \sum_{i=1}^{n} l(f(x_i), y_i)\]
empirical risk minimizer: \[
	\hat{f} = \argmin_{f} \hat{R}_n(f)
\] 
hypothesis space: $F$ \\
constrained empirical risk minimizer: \[
	\hat{f}_{n} = \argmin_{f \in F} \hat{R}
\] \\
constrained risk minimizer: \[
	f^{\ast}_{F} = \argmin_{f \in F} R
.\] 

\section{Define SVM}

{\noindent input space: $R^d$} \\
action space: $R$ \\
output space: $\{-1, 1\}$\\
hypothesis space: \[ F = \{ f(x) = w^T x | w \in R^d\}\]
loss/hinge loss: \[ l\left( f(x), y \right) = \max(1 - y_if(x_i), 0) = \left( 1 - y_if(x_i) \right)_+ \]
l2 regularization: \[
	min_{w \in R^d}\sum_{i=1}^{n} \left( 1 -y_if_w(x_i) \right)_+ + \lambda\|w\|_2^2 
,\] 
How does minimizing w relates to minimizing the risk function? draft*\\
Can we derive the loss function from a probability distribution like we did for logistic regression? draft*\\
$f_w(x_i)$ is called the score, and $y_if_w(x_i)$ is called the margin.

graph of logistic loss, hinge loss, perceptron loss, zero-one loss, square loss with respect to margin draft*
\section{Duality}
\section{Kernel}
\chapter{Tree Based Models}

\chapter{Text Processing}
The text analytic workflow: \\ \linebreak
raw data $\rightarrow$ word segmentation $\rightarrow$ cleaning (stopwords, lowercase, special character) $\rightarrow$ normalization (stemming, lemmazation) $\rightarrow$ feature extraction (tf-idf, word2vec) $\rightarrow$ modeling (classification, similarity measures)

\section{Segmentation}
\subsection{Tools for Chinese words segmentation}
\begin{itemize}
	\item Jieba: https://github.com/fxsjy/jieba
	\item SnowNLP: https://github.com/isnowfy/snownlp
	\item LTP: http://www.ltp-cloud.com/
	\item HanNLP: https://github.com/hankcs/HanLP/
\end{itemize}

\subsection{Max matching algorithm}
{\noindent sentence: 经常有意见分歧}\\
dictionary: [“我们”，“经常”，“有”，“有意见”， “意见”， “分歧”]\\
window size: 5

\begin{itemize}
	\item forward-max matching:\\
		经常有意见\\
		经常有意\\
		经常有\\
		经常 （check）
	\item backward-max matching:\\
		有意见分歧\\
		意见分歧\\
		见分歧\\
		分歧 （check）
\end{itemize}
Both approaches gives 经常|有意见|分歧, which is not a meaningful split.

\subsection{Segmentation considering semantic links}
To get a semantically meaningful split, we use language model. For example, the unigram model. To use the model, calculate the probability of the whole sentence P( 经常有意见分歧) for every possible way of segmentation: \\ 
by iid:\\
P(经常|有意见|分歧) = P(经常) * P(有意见) * P(分歧) \\
P(经常|有|意见|分歧) = P(经常) * P(有) * P(意见) * P(分歧),\\
calculating the product of small numbers is not possible because of limited floating point precision, so we apply log transformation.

Another problem is that, for a large text, there can be lots of ways to segment it, in this case we use the Viterbi algorithm to reduce the complexity.

\section{Filtering of words}
Stop words.

Low frequency words. Dropping low frequency words helps reducing computation time. Usually 80\%(a majority) of words are low frequency.

\section{Normalization of words}
Lemmazation.

Stemming.

\section{Spell correction}
After segmentation, if we find some words are not in the dictionary, we use edit distance to correct these words. (How to segment a sentence if there is no corresponding words to a part of the sentence? What to do if multiple candidates have the same edit distance? draft*)

Implement weighted edit distance: draft*

To find the candidates for correction, we can ether compare a wrong word with all the other words in the dictionary, or, more efficiently, we generate `words' that are small distance away from the wrong word, then filter them according to the dictionary. Finally, we rank the rest candidates by the probability $\hat{c} = \argmax_{c \in candidates} p(c|s)$, by the Bayes theorem, $\hat{c} \propto \argmax_{c \in candidates} p(s|c) * p(c)$. The first term, the probability of typing the wrong word given the intended word is c, can be obtained from historical data. The second term, the probability of c appeared in the text, can be obtained from counting the number of appearance of the word in the corpus.

\href{https://github.com/li-aolong/li-aolong.github.io/issues/12}{Here} are a list of open source Chinese spell correction tools.

\chapter{Text Representation}
\section{TF-IDF}
To represent a text, there are several options. Boolean vector being one of them doesn't consider the number of occurrence of every word. Count vector does consider the occurrence, but it's not that the higher the frequency, the more important the word is. To classify several documents, if the same word appears in every document, then the word is not very useful for classifying that document. Conversely, those words appears only in certain types of documents are more useful for their classification.

Tf-idf can solve this problem. It is a great baseline, and often better than neural networks. It is defined as: \[
	tf\text{-}idf(t, d, D) = tf(t, d) \times idf(t, D)
.\] 
Term frequency is the same thing as the count vector, defined as the number of occurrence of a term t in a document d. Document frequency is the size of a subset of all the documents D where all the documents containing a term t. We divide document frequency by the total number of all the documents D to get inverse document frequency. 
See \href{https://stackoverflow.com/q/27067992/9851286}{the effect of the log function in idf}. See \href{https://stats.stackexchange.com/q/166812/354019}{Why add 1 to idf}.
Idf is the same for every document.

\section{Text similarity}
\begin{itemize}
	\item Euclidean distance
		\[
		d(x, y) = \sqrt{(x_1 - y_1)^ + (x_2 - y_2)^2 + \ldots + (x_n - y_n)^2 } 
		.\] 
	To visualize Euclidean distance, think about Pythagorean theorem. 

	Euclidean distance doesn't consider vector direction, but vector has both magnitude and direction.
	\item Cosine distance\\
	Contrary to Euclidean distance, cosine distance only considers direction. It measures the angle between the vectors. \[
	d(x, y) = \frac{x \cdot y}{\|x\|\|y\|}
	.\] 
	To understand this formula, think about the geometric interpretation of dot product: $x \cdot y = \|x\|\|y\|\cos\theta$.
\end{itemize}

\section{Word2Vec}
We talked about how to represent text, we will now talk about word representation. We could represent words by one-hot encoding:
{\center
“我们”: [1, 0, 0]\\
“和”: [0, 1, 0]\\
“他们”: [0, 0, 1]\\}
But how to calculate the similarities between two words? The Euclidean distance of every pair of different words is $\sqrt{2} $, the cosine distance is $0$. 

We introduce word vector. While one-hot matrix is sparse, word vector is dense, but every word vector has lesser elements than word vector, because the size of a one-hot vector is the number of all the words of a text, can be millions, while the size of every word vector is a hyperparamter that we can tune. Normally, the more words we have, the longer the vectors we could set, but long vectors can leads to overfitting.

To evaluate word vectors, we can use some dimension reduction method and visualize them.

Once we have the word vectors, we can use them to represent sentence. One simple way is to calculate the average of the vectors of the words appeared in the sentence (bag of words).

The purpose of word vector is to quantify meaning. We have developed several ways of represent meaning, for example, denotational meaning, WordNet, and the aforementioned one-hot vector, but the most successful one for the computer to grasp is the distributional semantics, which says: a word's meaning is given by the words that frequently appear close-by.
{\center
``You shall know a word by the company it keeps'' (J.R.Firth 1957:11)}


\subsection{SkipGram}
SkipGram or Skip-NGram, opposite to CBOW(Continuous Bag of Words) which uses surrounding words to predict a center word, uses a center word to predict surrounding words. The number of surrounding words is a hyperparameter we can tune, usually between 5 and 20.

The objective function is \[
\prod_{t=1}^{T} \prod_{-m\le c \le m, c\neq 0} P(w_{t + c} | w_t; \theta)   
,\] 
where $w$ stands for word, $T$ is the total number of words, $m$ is the window size, and $\theta$, is the only parameter (not the only hyperparameter) we need to learn. It consists of 2 sets of vectors, a vector for each word as a context word, and a vector for each word as a center (target) word. 

As an example, the objective function of the sentence ``We love natural language processing'' with the window size equal to 1 is:\\ 
	P(love | we)P(we | love)P(natural | love)P(love | natural)P(language | natural)P(natural | language)P(processing | language)P(language | processing) parameterized by context vector and center word vector of each word. \\
	By using 2 kinds of vector for each word, we significantly simplify the calculation. Otherwise we have to apply Bayes theorem to calculate, for example, P(love | we) and P(we | love). 

	We don't consider the distance between a context word and the center word. There are other algorithms that consider this, and they are useful for syntax parsing, but for semantics, position doesn't matter a lot.

	Taking the negative log of the objective function, and apply the average of the function (\href{https://stats.stackexchange.com/q/267847/354019}{see the motivation of averaging the llh}), we get: \[
	-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \le c \le m, c \neq 0} \log P(w_{t+c} | w_t; \theta)	
	.\]

	Now, what is the function of $P(w_{t+c} | w_t; \theta)$? For each center (target) word, the conditional probability must sum to 1, and for any number, to make it positive, we apply the exponential transform. The result is the softmax function: 
	\[
		\frac{e^{u_{c + t} \cdot v_t}}{\sum_{c' = -m, c' \neq 0}^{m} e^{u_{c' + t} \cdot v_t}}
	,\] where $u_c$ and $u_{c'}$ are the context vector of a word, $v_t$ is the vector of the target word. (Why use dot product but not cosine similarity? Why apply softmax to the output layer of the neural net? How to map our function to layers of the neural net? draft* ) \href{https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling}{Demystifying Neural Network in Skip-Gram Language Modeling}

Now we calculate the gradient:
\begin{align*}
	\frac{\partial}{\partial v_w} \log
	\frac{e^{u_o \cdot v_w}}{\sum_{x} e^{u_x \cdot v_w}} &= \frac{\partial}{\partial v_w} \log e^{u_o \cdot v_w} - \log \sum_{x} e^{u_x \cdot v_w} \\
									  &= \frac{\partial }{\partial v_w} u_o \cdot v_w -  \log \sum_{x} e^{u_{x} \cdot v_w} \\
									  &= u_o - \frac{\partial }{\partial v_w} \log \sum_{x} e^{u_x \cdot v_w} \\
									  &= u_o - \frac{1}{\sum_{x} e^{u_x \cdot v_w}} \frac{\partial }{\partial v_w} \sum_{y} e^{u_y \cdot v_w} \\
									  &= u_o - \frac{1}{\sum_{x} e^{u_x \cdot v_w}}  \sum_{y} \frac{\partial }{\partial v_w} e^{u_y \cdot v_w} \\ 
									  &= u_o - \frac{1}{\sum_{x} e^{u_x \cdot v_w}}  \sum_{y} e^{u_y \cdot v_w} u_y \\ 
									  &= u_o - \sum_{y}  \frac{e^{u_y \cdot v_w}}{\sum_{x} e^{u_x \cdot v_w}} u_y \\
									  &= u_o - \sum_{y} softmax(u_y \cdot v_w) u_y \\
									  &= u_o - \sum_{y} P(y|w) u_y
,\end{align*}
which is the difference between the context word and the expected context word.
\subsection{Negative sampling \& hierarchical softmax}

\section{Other word2vec algorithms}
\subsection{Matrix factorization}
\subsection{GloVe}
\subsection{Gaussian embedding}

\chapter{Language Model}
\end{document}

