\documentclass{report}

\title{NLP Study Notes}
\author{Du Jinrui}
\date{2022}
\usepackage[b6paper, margin=0.5in]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
  \maketitle
  \tableofcontents
  \chapter{Shortest-Path Algorithms and Dynamic Programming}
  \section{Graphs}
  \section{Dynamic programming}
  When designing a DP algorithm, there are two things to consider:
  \begin{enumerate}
  	\item Deconstruct a big problem into smaller (recursive) sub-problems.
	\item Store intermediate results.
  \end{enumerate}
  \subsection{DP coding problems}
  \begin{itemize}
    \item Nth Fibonacci Number
    \item Longest Increasing Sub-sequence
    \item Coin Change
  \end{itemize}
  \section{The Viterbi algorithm}

  \chapter{Logistic Regression}
  \section{The importance of establishing a baseline}
  We draw a function that shows decreased marginal accuracy with increasing model complexity. From this graph, we observe an upper limit. This limit helps us making informed decisions like:
		\begin{enumerate}
			\item Is this project feasible? (the requirement is 75\% accuracy but the upper limit is 72\%.)
			\item Is it cost-effective to add model complexity?
		\end{enumerate}
Furthermore, if we use a complex model upfront without setting a baseline but the accuracy is bad, then it's hard for us to tell whether there was a mistake when building the model or it's because the problem is too complex.

  \section{Understanding LR}
  graph of 1d data draft*

  \href{https://towardsdatascience.com/why-sigmoid-a-probabilistic-perspective-42751d82686}{Why sigmoid?}
  \section{From likelihood to cost function}
  The likelihood function is defined as $l\left( \theta|D \right) = f\left( D|\theta\right) $. $f$ can be either a PMF or a PDF. $|$ is used instead of $;$ because we employ the Bayesian view (not frequenist) and see $\theta$ as a random variable. $l$ is a function of $\theta$ and doesn't integrate to 1 (with respect to $\theta$).

The likelihood function of logistic regression is 
\begin{displaymath}
\prod_{i=1}^{n} {\sigma\left( wx_{i} + b \right)^{y_{i}}\left( 1 - \sigma\left( wx_{i} + b\right)\right)^{1 - y_{i} }}.
\end{displaymath} (\href{https://zstevenwu.com/courses/s20/csci5525/resources/slides/lecture05.pdf}{see derivation})
Maximizing the likelihood is equal to minimizing the negative log-likelihood: \[
	cost\left( w, b \right) = -\sum_{i=1}^{n} y_{i} \ln \sigma \left(wx_{i} + b\right) + \left( 1 - y_{i} \right) \ln \left(1 - \sigma\left(wx_{i} + b\right)\right)
.\] 
And we get KL divergence, or binary cross-entropy, which is convex. (Why is it convex? And what is the difference between kl divergence and cross-entropy? draft*)
  \section{Implement LR with mini-batch GD}The cost function can't be solved analytically, hence we use gradient descent.
  The derivative of the sigmoid function is:
  \[
  \sigma(x)(1 - \sigma(x))
  .\] 
  Knowing this facilitates the calculation of the gradient: 
  \begin{align*}
	  \frac{\partial l(w, b)}{\partial w} & = \sum_{i = 1}^{n} (\sigma(wx_i + b) - y_i) x_i \\
\frac{\partial l(w, b)}{\partial b} & = \sum_{i = 1}^{n} \sigma(wx_i + b) - y_i. 
  \end{align*}
  Now we update the parameters:
  \begin{align*}
    w^{t+1} &= w^t - \eta_t \sum_{i=1}^{n} (\sigma(wx_i + b) - y_i) x_i \\
    b^{t+1} &= b^t - \eta_t \sum_{i=1}^{n} \sigma(wx_i + b) - y_i
  .\end{align*}
Now we've got the updates using GD. The updates using mini-batch GD and stochastic GD become apparent. The former is:
  \begin{align*}
    w^{t+1} &= w^t - \eta_t \sum_{x_i, y_i \in batch}(\sigma(wx_i + b) - y_i) x_i \\
    b^{t+1} &= b^t - \eta_t \sum_{x_i, y_i \in batch}\sigma(wx_i + b) - y_i
  .\end{align*}

  Between GD and stochastic GD, mini-batch GD finds the balance between robustness and efficiency. Moreover, it works well with GPU, and it helps escaping the saddle point. 

  code draft*
  \chapter{Generalization}
  \section{When w goes to infinity}
  When the problem is linearly separable, as w goes to infinity:
 \begin{align*}
	\lim_{w \to \infty} p\left( y_{i}=1 | x_{i} ; w,b \right) &=  \lim_{w \to \infty} \frac{1}{1 + e^{-(wx_{i} + b)} } = 1 \ for\ wx_i + b > 0,\\
	\lim_{w \to \infty} p( y_{i}=0 | x_{i} ; w,b ) &= \lim_{w \to \infty} \frac{e^{-(wx_{i}+b)}}{1 + e^{-(wx_{i} + b)}} = 0\ for\ wx_i + b < 0 
.\end{align*}
At this time, MLE is the largest:
 \[
	 MLE = \argmax_{w, b} \prod_{i=1}^{n}  p\left( y_{i}=1 | x_{i} ; w,b \right)^{y_{i}}p\left( y_{i}=0 | x_{i} ; w,b \right)^{1 - y_{i}}  
.\] 
It is consistent with our goal of maximizing the likelihood function to aim for a large w. For a linearly separable problem, w doesn't converge, and regularization gives bounded solution.

For a non-linearly separable problem, w can converge (mathematically, why?). But when there are too many features, the non-separable becomes the separable, again, w goes to infinity, and uncertainty regions shrink to 0. At this point, limiting the magnitude of w leads to better generalization  and gives back uncertainty regions. How are all these happening? \href{https://stackoverflow.com/questions/24666312/why-limit-the-weight-size-can-prevent-overfitting-in-machine-learning}{1} \href{https://stackoverflow.com/questions/34569903/how-does-having-smaller-values-for-parameters-help-in-preventing-over-fitting}{2} Graphically, higher degree terms variables with smaller w doesn't disappear, but go `out of range', e.g. $y = 6x_1 + 3x_2^2$ vs $y = 6x_1 + 0.1x_2^2$. draft* 

\href{https://stats.stackexchange.com/q/569381/354019}{We don't discuss feature selection here, why don't we just use feature selection?} Is there an algorithm for separability testing?
\section{L1 and L2 regularization}
3d geometric moving representation of l1 and l2 and why l1 makes some parameters 0. draft*

There are some disadvantages of l1 regularization:
\begin{enumerate}
	\item It's not differentiable everywhere, so gradient descent doesn't work, in this case we can use subgradient descent (I don't need to know the details).
	\item When a group of collinear features exist, it randomly selects one feature, but we want the best feature. The lecturer says using elastic net can counter this problem but I don't know how. It's another topic. draft*
\end{enumerate}

\section{K-fold CV}
When dataset is small, we can increase k. One extreme case is leave-one-out CV.

\section{MLE, MAP and L1, L2}
MLE: \[
	p(D|\theta)
.\] 
MAP:
\[
	p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)} \propto p(D|\theta)p(\theta)
.\] 
MAP estimator: \[
	\theta_{MAP} = \argmax_{\theta}\ prior\ \cdot\ likelihood
.\] 
Assume prior is $p(\theta) \sim N(0, \sigma^2)$,
\begin{align*}
	p(\theta) &= \frac{1}{\sqrt{2\pi} \sigma} \exp \left( -\frac{\theta^2}{2\sigma^2} \right) \\
		  &\propto \exp \left( -\frac{
		  \theta^2}{2\sigma^2} \right), \\
		  \argmax_{\theta}\ \log(p(\theta)) &= \argmax_{\theta}\ \log \left( \exp \left( -\frac{\theta^2}{2\sigma^2} \right) \right) \\ 
 &= \argmax_{\theta}\ -\frac{\theta^2}{2\sigma^2}, \\
		  \theta_{MAP} &= \argmin_{\theta} - \log \ likelihood +  \frac{1}{2\sigma^2} \theta^2
.\end{align*}
This looks very familiar. MAP estimator with Gaussian prior equals adding a l2 regularization term to the cost function (and how does the $\lambda$ coefficient relates to the variance? draft*).

Similarly when $p(\theta) \sim Laplace(0, b)$, the resulting cost function is added by l1 term.
\end{document}
